{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb6d38c2",
   "metadata": {},
   "source": [
    "# Procedural Programming\n",
    "\n",
    "The code counts the number of times a song appears in the log_of_songs variable. \n",
    "\n",
    "You'll notice that the first time you run `count_plays(\"Despacito\")`, you get the correct count. However, when you run the same code again `count_plays(\"Despacito\")`, the results are no longer correct.This is because the global variable `play_count` stores the results outside of the count_plays function. \n",
    "\n",
    "\n",
    "# Instructions\n",
    "\n",
    "Run the code cells in this notebook to see the problem with  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "060e6067",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_of_songs = [\n",
    "        \"Despacito\",\n",
    "        \"Nice for what\",\n",
    "        \"No tears left to cry\",\n",
    "        \"Despacito\",\n",
    "        \"Havana\",\n",
    "        \"In my feelings\",\n",
    "        \"Nice for what\",\n",
    "        \"Despacito\",\n",
    "        \"All the stars\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d8fecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c75152ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_plays(song_title):\n",
    "    global play_count\n",
    "    for song in log_of_songs:\n",
    "        if song == song_title:\n",
    "            play_count = play_count + 1\n",
    "    return play_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34aa4c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_plays(\"Despacito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1ba2906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_plays(\"Despacito\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc5837",
   "metadata": {},
   "source": [
    "# How to Solve the Issue\n",
    "\n",
    "How might you solve this issue? You could get rid of the global variable and instead use play_count as an input to the function:\n",
    "\n",
    "```python\n",
    "def count_plays(song_title, play_count):\n",
    "    for song in log_of_songs:\n",
    "        if song == song_title:\n",
    "            play_count = play_count + 1\n",
    "    return play_count\n",
    "\n",
    "```\n",
    "\n",
    "How would this work with parallel programming? Spark splits up data onto multiple machines. If your songs list were split onto two machines, Machine A would first need to finish counting, and then return its own result to Machine B. And then Machine B could use the output from Machine A and add to the count.\n",
    "\n",
    "However, that isn't parallel computing. Machine B would have to wait until Machine A finishes. You'll see in the next parts of the lesson how Spark solves this issue with a functional programming paradigm.\n",
    "\n",
    "In Spark, if your data is split onto two different machines, machine A will run a function to count how many times 'Despacito' appears on machine A. Machine B will simultaneously run a function to count how many times 'Despacito' appears on machine B. After they finish counting individually, they'll combine their results together. You'll see how this works in the next parts of the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584bcbd9",
   "metadata": {},
   "source": [
    "# Maps\n",
    "\n",
    "In Spark, maps take data as input and then transform that data with whatever function you put in the map. They are like directions for the data telling how each input should get to the output.\n",
    "\n",
    "The first code cell creates a SparkContext object. With the SparkContext, you can input a dataset and parallelize the data across a cluster (since you are currently using Spark in local mode on a single machine, technically the dataset isn't distributed yet).\n",
    "\n",
    "Run the code cell below to instantiate a SparkContext object and then read in the log_of_songs list into Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a3f4668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/18 17:16:24 WARN Utils: Your hostname, Lucianos-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.191 instead (on interface en0)\n",
      "22/09/18 17:16:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/18 17:16:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "### \n",
    "# You might have noticed this code in the screencast.\n",
    "#\n",
    "# import findspark\n",
    "# findspark.init('spark-2.3.2-bin-hadoop2.7')\n",
    "#\n",
    "# The findspark Python module makes it easier to install\n",
    "# Spark in local mode on your computer. This is convenient\n",
    "# for practicing Spark syntax locally. \n",
    "# However, the workspaces already have Spark installed and you do not\n",
    "# need to use the findspark module\n",
    "#\n",
    "###\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"maps_and_lazy_evaluation_example\").getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "sc.setLogLevel(\"FATAL\")\n",
    "\n",
    "log_of_songs = [\n",
    "        \"Despacito\",\n",
    "        \"Nice for what\",\n",
    "        \"No tears left to cry\",\n",
    "        \"Despacito\",\n",
    "        \"Havana\",\n",
    "        \"In my feelings\",\n",
    "        \"Nice for what\",\n",
    "        \"despacito\",\n",
    "        \"All the stars\"\n",
    "]\n",
    "\n",
    "# parallelize the log_of_songs to use with Spark\n",
    "distributed_song_log = sc.parallelize(log_of_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65b8b6",
   "metadata": {},
   "source": [
    "This next code cell defines a function that converts a song title to lowercase. Then there is an example converting the word \"Havana\" to \"havana\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85645292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'havana'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_song_to_lowercase(song):\n",
    "    return song.lower()\n",
    "\n",
    "convert_song_to_lowercase(\"Havana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b5ad4",
   "metadata": {},
   "source": [
    "The following code cells demonstrate how to apply this function using a map step. The map step will go through each song in the list and apply the convert_song_to_lowercase() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67aab8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.map(convert_song_to_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b283338e",
   "metadata": {},
   "source": [
    "You'll notice that this code cell ran quite quickly. This is because of lazy evaluation. Spark does not actually execute the map step unless it needs to.\n",
    "\n",
    "\"RDD\" in the output refers to resilient distributed dataset. RDDs are exactly what they say they are: fault-tolerant datasets distributed across a cluster. This is how Spark stores data. \n",
    "\n",
    "To get Spark to actually run the map step, you need to use an \"action\". One available action is the collect method. The collect() method takes the results from all of the clusters and \"collects\" them into a single list on the master node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c491e8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['despacito',\n",
       " 'nice for what',\n",
       " 'no tears left to cry',\n",
       " 'despacito',\n",
       " 'havana',\n",
       " 'in my feelings',\n",
       " 'nice for what',\n",
       " 'despacito',\n",
       " 'all the stars']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.map(convert_song_to_lowercase).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1191f",
   "metadata": {},
   "source": [
    "Note as well that Spark is not changing the original data set: Spark is merely making a copy. You can see this by running collect() on the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00d49ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Despacito',\n",
       " 'Nice for what',\n",
       " 'No tears left to cry',\n",
       " 'Despacito',\n",
       " 'Havana',\n",
       " 'In my feelings',\n",
       " 'Nice for what',\n",
       " 'despacito',\n",
       " 'All the stars']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0dac67",
   "metadata": {},
   "source": [
    "You do not always have to write a custom function for the map step. You can also use anonymous (lambda) functions as well as built-in Python functions like string.lower(). \n",
    "\n",
    "Anonymous functions are actually a Python feature for writing functional style programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c7fa01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despacito',\n",
       " 'nice for what',\n",
       " 'no tears left to cry',\n",
       " 'despacito',\n",
       " 'havana',\n",
       " 'in my feelings',\n",
       " 'nice for what',\n",
       " 'despacito',\n",
       " 'all the stars']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.map(lambda song: song.lower()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd658586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despacito',\n",
       " 'nice for what',\n",
       " 'no tears left to cry',\n",
       " 'despacito',\n",
       " 'havana',\n",
       " 'in my feelings',\n",
       " 'nice for what',\n",
       " 'despacito',\n",
       " 'all the stars']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributed_song_log.map(lambda x: x.lower()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed0ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
